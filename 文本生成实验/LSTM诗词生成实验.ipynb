{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM诗词生成实验\n",
    "\n",
    "文本生成是自然语言处理领域的一个重要研究方向，研究人员期待有一天计算机能够像人类一样会表达，能够撰写出高质量的自然语言文本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目的\n",
    "\n",
    "本章的主要内容就是基于 MindSpore 实现文本生成，通过本实验，学员可以学习LSTM网络的使用，文本的预处理操作。熟悉循环神经网络的结构，文本生成的过程以及网络模型的训练和预测，掌握 MindSpore 的相关操作等。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验环境\n",
    "\n",
    "ModelArts Notebook环境，目前仅支持GPU/Ascend，具体参考实验正文。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "本文使用了2930首五言律诗作为训练数据集，每一行数据为一首诗，这里使用的诗的作者和诗名都已经被删去。\n",
    "\n",
    "数据集保存在文件 `poetry.txt` 中，其中的部分古诗如下：\n",
    "\n",
    "```text\n",
    "山山浮翠远，处处落红深。独立柴门外，长歌托素心。\n",
    "翠华重幸日，瑞气霭龙牙。元老频承宠，宸章特赐嘉。\n",
    "调和归静穆，暑雨绝咨嗟。共仰明良会，唐虞岂有加。\n",
    "一字同华衮，天香遍齿牙。光辉悬日月，矍铄入褒嘉。\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集下载\n",
    "\n",
    "使用 `download` 接口下载数据集，并将下载后的数据集自动解压到当前目录下。数据下载之前需要使用 `pip install download` 安装 `download` 包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T02:05:27.956588Z",
     "start_time": "2023-04-04T02:05:22.301871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/LSTM_generate_poem_data.zip (3.1 MB)\n",
      "\n",
      "file_sizes: 100%|███████████████████████████| 3.20M/3.20M [00:03<00:00, 829kB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from download import download\n",
    "\n",
    "url = \"https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/LSTM_generate_poem_data.zip\"\n",
    "\n",
    "download(url, \"./\", kind=\"zip\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "先按行读取数据集，再使用 `rstrip` 方法去除后缀字符，然后过滤掉含有特殊字符的文本，然后用 `word2vec` 将文本词向量化。\n",
    "\n",
    "### Work2Vec\n",
    "\n",
    "Word2Vec 是一种将词转为向量的方法，是在2013年的论文 [Efficient Estimation of Word Representations inVector Space](https://arxiv.org/pdf/1301.3781.pdf) 中提出的，作者来自google。\n",
    "\n",
    "比较早出现的将词转为向量的方法是独热编码（One-Hot）。使用独热编码构建独热向量很容易，但通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度，而且在大量数据的情况下会出现数据的维度灾难。\n",
    "\n",
    "Word2Vec 方法刚好克服了独热编码的缺陷。它将每个词映射到一个固定长度（长度为可设置的超参数）的向量，这些向量能很好地表达不同词之间的相似性和类比关系。该方法包含两种模型算法：skip-gram 和 CBOW，它们的最大区别是 skip-gram 是通过中心词去预测中心词周围的词，而 CBOW 是通过周围的词去预测中心词。\n",
    "\n",
    "此方法需要通过 `gensim.models.word2vec` 来使用，上述的两种训练模型可以通过超参数 `sg` 来设置，默认为 CBOW。这里介绍 Work2Vec 的原因是它可以替换下文的模型的 Embedding 层。\n",
    "\n",
    "> `gensim` 库需手动安装，命令如下：`pip install gensim`\n",
    "\n",
    "具体情况见如下代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T02:08:28.621518Z",
     "start_time": "2023-04-04T02:08:28.595587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  4436\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mindspore as ms\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "dataset_path = './LSTM_generate_poem_data/poetry_5.txt'\n",
    "vec_params_file = 'vec_params_5.pkl'  # 保存词向量训练后的权重文件名字\n",
    "\n",
    "ms.set_context(device_target='CPU', mode=ms.PYNATIVE_MODE, device_id=0)\n",
    "\n",
    "def data_pre(dataset_path, vec_params_file):\n",
    "    # 特殊字符列表\n",
    "    forbidden_words = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "    poetry_src = []\n",
    "    with open(dataset_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for poem in lines:\n",
    "        forbidden_poem = [word in poem for word in forbidden_words]\n",
    "        # 过滤掉含有特殊字符的诗句\n",
    "        if sum(forbidden_poem) > 0:\n",
    "            continue\n",
    "        # 去除行尾换行符\n",
    "        poem = poem.rstrip()\n",
    "        poetry_src.append(poem)\n",
    "\n",
    "    if os.path.exists(vec_params_file):\n",
    "        return poetry_src, pickle.load(open(vec_params_file, \"rb\"))\n",
    "\n",
    "    model = Word2Vec(poetry_src, vector_size=100, min_count=1, workers=6)\n",
    "    pickle.dump((model.syn1neg, model.wv.key_to_index, model.wv.index_to_key), open(vec_params_file, \"wb\"))\n",
    "\n",
    "    return poetry_src, (model.syn1neg, model.wv.key_to_index, model.wv.index_to_key)\n",
    "\n",
    "all_data, (word_vec, word_2_index, index_2_word) = data_pre(dataset_path, vec_params_file)\n",
    "vocab_size, embedding_dim = word_vec.shape\n",
    "print('vocab_size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集\n",
    "\n",
    "完成预处理操作后，需将其加入到数据集处理流水线中。类方法 `PoetryDataGenerator` 将数据拆分为模型的输入数据和用于计算loss的标签，并转换为 Tensor 格式，`__getitem__` 方法将数据变为可迭代对象。函数 `create_poetry_dataset` 是将可迭代的数据使用 `GeneratorDataset` 封装成MindSpore 的 dataset 对象，然后可以使用dataset 对象的 `get_batch_size` 属性获得数据集的大小 ，使用 `batch` 批操作将数据批量化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "\n",
    "class PoetryDataGenerator(object):\n",
    "    def __init__(self, all_data, word_vec, word_2_index):\n",
    "        self.all_data = all_data\n",
    "        self.word_vec = word_vec\n",
    "        self.word_2_index = word_2_index\n",
    "\n",
    "    # 获取一条数据, 并作处理\n",
    "    def __getitem__(self, index):\n",
    "        a_poetry_words = self.all_data[index]\n",
    "        a_poetry_index = [self.word_2_index[word] for word in a_poetry_words]\n",
    "        xs_index = a_poetry_index[:-1]\n",
    "        ys_index = a_poetry_index[1:]\n",
    "        xs_embedding = self.word_vec[xs_index]\n",
    "\n",
    "        return Tensor(xs_embedding), Tensor(ys_index, ms.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "def create_poetry_dataset(batch_size, all_data, word_vec, word_2_index):\n",
    "    dt = PoetryDataGenerator(all_data, word_vec, word_2_index)\n",
    "    de = ds.GeneratorDataset(dt, [\"inputs\", \"label\"], shuffle=True)\n",
    "    size = de.get_dataset_size()\n",
    "    de = de.batch(batch_size, drop_remainder=True)\n",
    "    return de, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data size: (64, 23, 100)\n",
      "Size: 2930\n",
      "Batchsize: 64\n",
      "Total_step: 45\n"
     ]
    }
   ],
   "source": [
    "from mindspore import Tensor\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset, size = create_poetry_dataset(batch_size, all_data, word_vec, word_2_index)\n",
    "total_step = dataset.get_dataset_size()\n",
    "data1 = next(dataset.create_dict_iterator())\n",
    "data1_shape = data1[\"inputs\"].shape\n",
    "\n",
    "print('Input data size:', data1_shape)\n",
    "print('Size:', size)\n",
    "print('Batchsize:', batch_size)\n",
    "print('Total_step:', total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建\n",
    "\n",
    "完成数据集的处理后，我们设计用于文本生成的模型结构。一般情况下首先需要使用`nn.Embedding`层加载词向量；然后使用RNN循环神经网络做特征提取；最后将RNN连接至一个全连接层，即 `nn.Dense` ，将特征转化为需要预测的词表大小的结果，用于后续进行模型优化训练。整体模型结构如下：\n",
    "\n",
    "```text\n",
    "nn.Embedding -> nn.RNN -> nn.Dense\n",
    "```\n",
    "\n",
    "这里我们使用能够一定程度规避RNN梯度消失问题的变种 LSTM(Long short term memory) 做特征提取层。下面对模型进行详解：\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Embedding 层又可称为 EmbeddingLookup 层，其作用是使用 index id 对权重矩阵对应id的向量进行查找，当输入为一个由 index id 组成的序列时，则查找并返回一个相同长度的矩阵，例如：\n",
    "\n",
    "```text\n",
    "embedding = nn.Embedding(1000, 100) # 词表大小(index的取值范围)为1000，表示向量的size为100\n",
    "input shape: (1, 16)                # 序列长度为16\n",
    "output shape：(1, 16, 100)\n",
    "```\n",
    "\n",
    "这里我们使用 Work2Vec 方法处理的数据来替代 Embedding 层，可以达到相同的效果。\n",
    "\n",
    "### RNN(循环神经网络)\n",
    "\n",
    "循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的神经网络。下图为RNN的一般结构：\n",
    "\n",
    "![RNN-0](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-RNN-0.png)\n",
    "\n",
    "> 图示左侧为一个RNN Cell循环，右侧为RNN的链式连接平铺。实际上不管是单个RNN Cell还是一个RNN网络，都只有一个Cell的参数，在不断进行循环计算中更新。\n",
    "\n",
    "由于RNN的循环特性，和自然语言文本的序列特性(句子是由单词组成的序列)十分匹配，因此被大量应用于自然语言处理研究中。下图为RNN的结构拆解：\n",
    "\n",
    "![RNN](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-RNN.png)\n",
    "\n",
    "RNN单个Cell的结构简单，因此也造成了梯度消失(Gradient Vanishing)问题，具体表现为RNN网络在序列较长时，在序列尾部已经基本丢失了序列首部的信息。为了克服这一问题，LSTM(Long short term memory)被提出，通过门控机制(Gating Mechanism)来控制信息流在每个循环步中的留存和丢弃。下图为LSTM的结构拆解：\n",
    "\n",
    "![LSTM](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-LSTM.png)\n",
    "\n",
    "本节我们选择LSTM变种而不是经典的RNN做特征提取，来规避梯度消失问题，并获得更好的模型效果。下面来看MindSpore中`nn.LSTM`对应的公式：\n",
    "\n",
    "$$h_{0:t}, (h_t, c_t) = \\text{LSTM}(x_{0:t}, (h_0, c_0))$$\n",
    "\n",
    "这里`nn.LSTM`隐藏了整个循环神经网络在序列时间步(Time step)上的循环，送入输入序列、初始状态，即可获得每个时间步的隐状态(hidden state)拼接而成的矩阵，以及最后一个时间步对应的隐状态。我们使用最后的一个时间步的隐状态作为输入句子的编码特征，送入下一层。\n",
    "\n",
    "> Time step：在循环神经网络计算的每一次循环，成为一个Time step。在送入文本序列时，一个Time step对应一个单词。因此在本例中，LSTM的输出$h_{0:t}$对应每个单词的隐状态集合，$h_t$和$c_t$对应最后一个单词对应的隐状态。\n",
    "\n",
    "> 以上内容借鉴MindSpore官方教程的应用实践案例：[RNN实现情感分类](https://www.mindspore.cn/tutorials/application/zh-CN/master/nlp/sentiment_analysis.html#rnn%E5%AE%9E%E7%8E%B0%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB)\n",
    "\n",
    "### Dense\n",
    "\n",
    "在经过LSTM编码获取句子特征后，将其送入一个全连接层，即`nn.Dense`，将特征维度变换为需要预测的词表大小，然后使用 `ops.argmax` 选取最大的结果的位置，再对应到词表找到相应的词汇即为模型预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:15:39.085773Z",
     "start_time": "2023-03-17T03:15:39.058846Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import nn, ops, Tensor, save_checkpoint\n",
    "\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "class LSTMPoetryModel(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, batch_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.LSTM(self.embedding_dim,\n",
    "                           self.hidden_dim,\n",
    "                           num_layers=self.n_layers,\n",
    "                           batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Dense(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def construct(self, inputs, h_0=None, c_0=None):\n",
    "        if h_0 == None or c_0 == None:\n",
    "            h_0 = Tensor(np.zeros((self.n_layers, self.batch_size, self.hidden_dim)), ms.float32)\n",
    "            c_0 = Tensor(np.zeros((self.n_layers, self.batch_size, self.hidden_dim)), ms.float32)\n",
    "\n",
    "        outputs, (h_0, c_0) = self.rnn(inputs, (h_0, c_0))\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = ops.reshape(outputs, (outputs.shape[0] * outputs.shape[1], outputs.shape[2]))\n",
    "        output = self.fc(outputs)\n",
    "        return output, h_0, c_0\n",
    "\n",
    "# 模型实例化\n",
    "rnn_net = LSTMPoetryModel(vocab_size, embedding_dim, hidden_dim, n_layers, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化器\n",
    "\n",
    "完成模型主体构建后，然后选择损失函数和优化器。针对本案例使用的文本数据的特性，即一个多分类问题，这里选择交叉熵损失函数 `nn.CrossEntropyLoss()`，优化器选择 `nn.Adam`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:15:41.418762Z",
     "start_time": "2023-03-17T03:15:41.393777Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.003\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = nn.Adam(rnn_net.trainable_params(), learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "训练过程主要是先前向计算损失，然后根据损失值计算梯度，再根据梯度更新模型参数，最后打印损失保存权重文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n",
      "Epoch:[  1/1000], step:[   0/  45], loss:8.397839 , time:2.591877s, lr:0.003000\n",
      "Epoch:[  1/1000], step:[  10/  45], loss:7.154466 , time:0.057803s, lr:0.003000\n",
      "Epoch:[  1/1000], step:[  20/  45], loss:6.991076 , time:0.057386s, lr:0.003000\n",
      "Epoch:[  1/1000], step:[  30/  45], loss:6.842004 , time:0.072192s, lr:0.003000\n",
      "Epoch:[  1/1000], step:[  40/  45], loss:6.909730 , time:0.058578s, lr:0.003000\n",
      "Epoch:[  1/1000], loss:7.165276 , time:5.382530s, \n",
      "Epoch:[  2/1000], step:[   0/  45], loss:6.711904 , time:0.122232s, lr:0.003000\n",
      "...\n",
      "Epoch:[999/1000], step:[  40/  45], loss:1.877362 , time:0.069371s, lr:0.003000\n",
      "Epoch:[999/1000], loss:1.907126 , time:3.458176s, \n",
      "Epoch:[1000/1000], step:[   0/  45], loss:1.786555 , time:0.115004s, lr:0.003000\n",
      "Epoch:[1000/1000], step:[  10/  45], loss:1.939893 , time:0.073283s, lr:0.003000\n",
      "Epoch:[1000/1000], step:[  20/  45], loss:2.090731 , time:0.070930s, lr:0.003000\n",
      "Epoch:[1000/1000], step:[  30/  45], loss:1.893512 , time:0.070538s, lr:0.003000\n",
      "Epoch:[1000/1000], step:[  40/  45], loss:1.931937 , time:0.070642s, lr:0.003000\n",
      "Epoch:[1000/1000], loss:1.914270 , time:3.489737s, \n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore import context\n",
    "\n",
    "total_epoch = 1000\n",
    "\n",
    "def forward_fn(datas):\n",
    "    output, h_0, c_0 = rnn_net(datas[\"inputs\"])\n",
    "    loss = loss_fn(output, datas[\"label\"].reshape(-1))\n",
    "    return loss\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, rnn_net.trainable_params())\n",
    "\n",
    "def train_step(data1):\n",
    "    loss, grads = grad_fn(data1)\n",
    "    opt(grads)\n",
    "    return loss\n",
    "\n",
    "def train():\n",
    "    print('Start training!')\n",
    "    epochs_loss = []\n",
    "    for epoch in range(total_epoch):\n",
    "        start = time.time()\n",
    "        steps_loss = []\n",
    "        for step, data in enumerate(dataset.create_dict_iterator()):\n",
    "            start1 = time.time()\n",
    "            loss = train_step(data)\n",
    "\n",
    "            steps_loss.append(loss.asnumpy())\n",
    "            end1 = time.time()\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch:[{int(epoch+1):>3d}/{int(total_epoch):>3d}], \"\n",
    "                      f\"step:[{int(step):>4d}/{int(total_step):>4d}], \"\n",
    "                      f\"loss:{loss.asnumpy():>4f} , \"\n",
    "                      f\"time:{(end1 - start1):>3f}s, \"\n",
    "                      f\"lr:{lr:>6f}\")\n",
    "        epochs_loss.append(np.mean(steps_loss))\n",
    "        end = time.time()\n",
    "        print(f\"Epoch:[{int(epoch + 1):>3d}/{int(total_epoch):>3d}], \"\n",
    "              f\"loss:{epochs_loss[epoch]:>4f} , \"\n",
    "              f\"time:{(end - start):>3f}s, \")\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            save_checkpoint(rnn_net, f\"./LSTM_poetry_{epoch+1}.ckpt\")\n",
    "\n",
    "    print('Training completed!')\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测\n",
    "\n",
    "最后我们设计一个预测函数，实现随机初始化一首诗的第一个字然后生成一首五言诗。具体代码如下:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    result = \"\"\n",
    "    word_index = np.random.randint(0, vocab_size, 1)[0]\n",
    "    result += index_2_word[word_index]\n",
    "    print('Start word:', result)\n",
    "\n",
    "    h_0 = Tensor(np.zeros((n_layers, 1, hidden_dim)), ms.float32)\n",
    "    c_0 = Tensor(np.zeros((n_layers, 1, hidden_dim)), ms.float32)\n",
    "\n",
    "    for i in range(data1_shape[1]):\n",
    "        inputs = Tensor([[word_vec[word_index]]])\n",
    "        output, h_0, c_0 = rnn_net(inputs, h_0, c_0)\n",
    "        word_index = int(ops.argmax(output))\n",
    "        result += index_2_word[word_index]\n",
    "\n",
    "    print('Predict result:', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: 斧\n",
      "Predict result: 斧有三秋啼，团乔数其手。虽锋荡回问，或此者中边。\n"
     ]
    }
   ],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "param_dict = load_checkpoint('LSTM_poetry_1000.ckpt')\n",
    "load_param_into_net(rnn_net, param_dict)\n",
    "eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms2.0b130",
   "language": "python",
   "name": "ms2.0b130"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
